{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re \n",
    "import itertools\n",
    "import heapq\n",
    "import operator\n",
    "import networkx as nx\n",
    "import copy\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from operator import mul\n",
    "from nltk.metrics import scores\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clean and Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_simple(text, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
    "    \n",
    "    punct = string.punctuation.replace('-', '')\n",
    "    # remove punctuation (preserving intra-word dashes)\n",
    "    text = ''.join(l for l in text if l not in punct)\n",
    "    # strip extra white space\n",
    "    text = re.sub(' +',' ',text)\n",
    "    # strip leading and trailing white space\n",
    "    text = text.strip()\n",
    "    # tokenize (split based on whitespace)\n",
    "    tokens = text.split(' ')\n",
    "    if pos_filtering == True:\n",
    "        # apply POS-tagging\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        # retain only nouns and adjectives\n",
    "        tokens_keep = []\n",
    "        for i in range(len(tagged_tokens)):\n",
    "            item = tagged_tokens[i]\n",
    "            if (\n",
    "            item[1] == 'NN' or\n",
    "            item[1] == 'NNS' or\n",
    "            item[1] == 'NNP' or\n",
    "            item[1] == 'NNPS' or\n",
    "            item[1] == 'JJ' or\n",
    "            item[1] == 'JJS' or\n",
    "            item[1] == 'JJR'\n",
    "            ):\n",
    "                tokens_keep.append(item[0])\n",
    "        tokens = tokens_keep\n",
    "    if remove_stopwords:\n",
    "        stpwds = stopwords.words('english')\n",
    "        # remove stopwords\n",
    "        tokens = [token for token in tokens if token not in stpwds]\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        # apply Porter's stemmer\n",
    "        tokens_stemmed = list()\n",
    "        tokens_unstemmed = list()\n",
    "        for token in tokens:\n",
    "            tokens_stemmed.append(stemmer.stem(token).lower())\n",
    "            tokens_unstemmed.append(token)\n",
    "\n",
    "    return(tokens_stemmed,tokens_unstemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_graph(terms, w):\n",
    "    # This function returns a directed, weighted networkx graph from a list of terms (the tokens from the pre-processed text) e.g., ['quick','brown','fox']\n",
    "    # Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'\n",
    "    \n",
    "    from_to = {}\n",
    "    \n",
    "    # create initial complete graph (first w terms)\n",
    "    w = min(w,len(terms))\n",
    "    terms_temp = terms[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    \n",
    "    new_edges = []\n",
    "    \n",
    "    for my_tuple in indexes:\n",
    "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "    \n",
    "    for new_edge in new_edges:\n",
    "        if new_edge in from_to:\n",
    "            from_to[new_edge] += 1\n",
    "        else:\n",
    "            from_to[new_edge] = 1\n",
    "    \n",
    "    # then iterate over the remaining terms\n",
    "    for i in xrange(w, len(terms)):\n",
    "        # term to consider\n",
    "        considered_term = terms[i]\n",
    "        # all terms within sliding window\n",
    "        terms_temp = terms[(i-w+1):(i+1)]\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in xrange(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "            \n",
    "        for try_edge in candidate_edges:\n",
    "        \n",
    "            # if not self-edge\n",
    "            if try_edge[1] != try_edge[0]:\n",
    "                \n",
    "                # if edge has already been seen, update its weight\n",
    "                if try_edge in from_to:\n",
    "                    from_to[try_edge] += 1\n",
    "                \n",
    "                # if edge has never been seen, create it and assign it a unit weight     \n",
    "                else:\n",
    "                    from_to[try_edge] = 1\n",
    "    \n",
    "    # create empty graph\n",
    "    g = nx.DiGraph()\n",
    "    \n",
    "    for edge in from_to :\n",
    "        g.add_edge(edge[0],edge[1],weight=from_to[edge])\n",
    "    \n",
    "    degree_dict = g.degree(weight='weight')\n",
    "    nx.set_node_attributes(g,'weight',degree_dict)\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Attraction Force Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_vector_getter(word, word_vectors):\n",
    "    try:\n",
    "        word_representation = word_vectors[word].reshape(1,-1)\n",
    "        return (word_representation)\n",
    "    except KeyError:\n",
    "        return (np.random.uniform(-0.25,0.25,300).reshape(1,-1))\n",
    "        \n",
    "def my_euclidean_distance(word1, word2, word_vectors):\n",
    "    distance = ed(my_vector_getter(word1, word_vectors),my_vector_getter(word2, word_vectors))\n",
    "    return (round(distance, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAF Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_graph_word_attraction(terms_stemmed,terms_unstemmed,w):\n",
    "    # This function returns a directed, weighted networkx graph from a list of terms (the tokens from the pre-processed text) e.g., ['quick','brown','fox']\n",
    "    # Edges are weighted based on term co-occurence, word2vec vector and the respective frequencies within a sliding window of fixed size 'w'\n",
    "    from_to={}\n",
    "    \n",
    "    # create initial complete graph (first w terms)\n",
    "    w = min(w,len(terms_stemmed))\n",
    "    terms_temp = terms_stemmed[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    \n",
    "    new_edges = []\n",
    "    \n",
    "\n",
    "    for my_tuple in indexes:\n",
    "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "    \n",
    "    for new_edge in new_edges:\n",
    "        if new_edge in from_to:\n",
    "            from_to[new_edge] += 1\n",
    "        else:\n",
    "            from_to[new_edge] = 1\n",
    "    \n",
    "    # then iterate over the remaining terms\n",
    "    for i in xrange(w, len(terms_stemmed)):\n",
    "        # term to consider\n",
    "        considered_term = terms_stemmed[i]\n",
    "        # all terms within sliding window\n",
    "        terms_temp = terms_stemmed[(i-w+1):(i+1)]\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in xrange(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "            \n",
    "        for try_edge in candidate_edges:\n",
    "        \n",
    "            # if not self-edge\n",
    "            if try_edge[1] != try_edge[0]:\n",
    "                \n",
    "                # if edge has already been seen, update its weight\n",
    "                if try_edge in from_to:\n",
    "                    from_to[try_edge] += 1\n",
    "                \n",
    "                # if edge has never been seen, create it and assign it a unit weight     \n",
    "                else:\n",
    "                    from_to[try_edge] = 1\n",
    "    \n",
    "    # create empty graph\n",
    "    min_attr = float(\"inf\")\n",
    "    edgelist = from_to.keys()\n",
    "    waf = {}\n",
    "    for edge in edgelist :\n",
    "        word1 = edge[0]\n",
    "        word2 = edge[1]\n",
    "        if word1 != word2 :\n",
    "            word1_freq = terms_stemmed.count(word1)\n",
    "            word2_freq = terms_stemmed.count(word2)\n",
    "            word1_unstemmed = terms_unstemmed[terms_stemmed.index(word1)]\n",
    "            word2_unstemmed = terms_unstemmed[terms_stemmed.index(word2)]\n",
    "            distance = my_euclidean_distance(word1_unstemmed,word2_unstemmed,word_vectors)\n",
    "            force = round(word1_freq * word2_freq / float(distance * distance), 5)\n",
    "            dice = 2*from_to[edge]/(word1_freq*word2_freq)\n",
    "            attr = dice*force\n",
    "            if attr!=0:\n",
    "                min_attr = min(attr,min_attr)\n",
    "                waf[edge] = attr\n",
    "    \n",
    "    g = nx.DiGraph()\n",
    "    for item in waf :\n",
    "        if waf[item]!=0:\n",
    "            waf[item] = round(waf[item]*1.0/min_attr)\n",
    "            g.add_edge(item[0],item[1],weight=waf[item])\n",
    "    \n",
    "    degree_dict = g.degree(weight='weight')\n",
    "    nx.set_node_attributes(g,'weight',degree_dict)\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def core_dec(g, weighted = True):\n",
    "    # unweighted or weighted k-core decomposition of a graph\n",
    "  \n",
    "    # work on clone of g to preserve g \n",
    "    gg = copy.deepcopy(g)    \n",
    "    \n",
    "    # initialize dictionary that will contain the core numbers\n",
    "    cores_g = dict((key,0) for key in gg.nodes())\n",
    "    \n",
    "    if weighted == True:\n",
    "        # k-core decomposition for weighted graphs (generalized k-cores)\n",
    "        # based on Batagelj and Zaversnik's (2002) algorithm #4\n",
    "    \n",
    "        # initialize min heap of degrees\n",
    "        degree_dict = gg.degree(weight='weight')\n",
    "        heap_g = [[degree_dict[node],node] for node in degree_dict]\n",
    "        heapq.heapify(heap_g)\n",
    "        while len(heap_g)>0:\n",
    "            \n",
    "            top = heap_g[0][1]\n",
    "            # save names of its neighbors\n",
    "            neighbors_top = gg[top].keys()\n",
    "            # set core number of heap top element as its weighted degree\n",
    "            cores_g[top] = gg.node[top]['weight']\n",
    "            \n",
    "            # delete top vertice (weighted degrees are automatically updated)\n",
    "            gg.remove_node(top)\n",
    "            \n",
    "            if len(neighbors_top)>0:\n",
    "            # iterate over neighbors of top element\n",
    "                for i, name_n in enumerate(neighbors_top):\n",
    "                    max_n = max(cores_g[top],gg.degree(weight='weight')[name_n])\n",
    "                    gg.node[name_n][\"weight\"] = max_n\n",
    "                    # update heap\n",
    "                    heap_g = [[gg.node[nodex]['weight'],nodex] for nodex in gg.nodes()]\n",
    "                    heapq.heapify(heap_g)\n",
    "            else:\n",
    "                # update heap\n",
    "                heap_g = [[gg.node[nodex]['weight'],nodex] for nodex in gg.nodes()]\n",
    "                heapq.heapify(heap_g)\n",
    "                \n",
    "    else:\n",
    "        # k-core decomposition for unweighted graphs\n",
    "        # based on Batagelj and Zaversnik's (2002) algorithm #1\n",
    "        cores_g = nx.core_number(gg)\n",
    "    \n",
    "    # sort vertices by decreasing core number\n",
    "#     sorted_cores_g = sorted(cores_g.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return cores_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted K-Peak Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kpeak_decomposition(G,weighted = True):\n",
    "    if weighted == True :\n",
    "        orig_core_nums = core_dec(G,weighted)\n",
    "        current_core_nums = dict(orig_core_nums)\n",
    "    else :\n",
    "        orig_core_nums = nx.core_number(G)\n",
    "        current_core_nums = orig_core_nums.copy()\n",
    "        \n",
    "    H = G.copy(); \n",
    "    H_nodes = set(G.nodes())\n",
    "    \n",
    "    \n",
    "    peak_numbers = {}\n",
    "\n",
    "    # Each iteration of the while loop finds a k-contour\n",
    "    if weighted == True :\n",
    "        while(len(H.nodes()) > 0):\n",
    "            temp_dict = dict(current_core_nums)\n",
    "            sorted_tracker = sorted(current_core_nums.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            degen_number = sorted_tracker[0][1]\n",
    "            kcontour_nodes = []\n",
    "            for x in current_core_nums :\n",
    "                if current_core_nums[x] == degen_number :\n",
    "                    kcontour_nodes.append(x)\n",
    "            for n in kcontour_nodes :\n",
    "                peak_numbers[n] = temp_dict[n]\n",
    "            # Removing the kcontour (i.e. degeneracy) and re-computing core numbers.\n",
    "            H_nodes = H_nodes.difference(set(kcontour_nodes))\n",
    "            H = G.subgraph(list(H_nodes))\n",
    "            current_core_nums = core_dec(H)\n",
    "    \n",
    "    else :\n",
    "        while(len(H.nodes()) > 0):\n",
    "            \n",
    "            # degen_core is the degeneracy of the graph\n",
    "            degen_core = nx.k_core(H) # Degen-core\n",
    "\n",
    "            # Nodes in the k-contour. Their current core number is their peak number.\n",
    "            kcontour_nodes = degen_core.nodes()\n",
    "            for n in kcontour_nodes:\n",
    "                peak_numbers[n] = current_core_nums[n]\n",
    "\n",
    "            # Removing the kcontour (i.e. degeneracy) and re-computing core numbers.\n",
    "            H_nodes = H_nodes.difference(set(kcontour_nodes))\n",
    "            H = G.subgraph(list(H_nodes))\n",
    "            current_core_nums = nx.core_number(H)\n",
    "    \n",
    "    # sort vertices by decreasing peak number\n",
    "#     sorted_peaks_g = sorted(peak_numbers.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return peak_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rank(core_numbers, g):\n",
    "    core_rank = {}\n",
    "    for node in core_numbers :\n",
    "        core_rank[node] = sum([core_numbers[neighbor] for neighbor in g[node].keys()])\n",
    "        #core_rank[node] = reduce(mul,[core_numbers[neighbor] for neighbor in g[node].keys()])\n",
    "        #core_rank[node] = sum([pow(core_numbers[neighbor],g[node][neighbor]['weight']) for neighbor in g[node].keys()])\n",
    "    sorted_ranks = sorted(core_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_ranks = [x[0] for x in sorted_ranks]\n",
    "    return sorted_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hulth Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = os.listdir(\"EMNLP_2016-master/data/Hulth2003/validation_training/validation/\")\n",
    "path = \"EMNLP_2016-master/data/Hulth2003/validation_training/validation/\"\n",
    "abstract_files = [filename for filename in total if '.abstr' in filename]\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "crp_normal_p,crp_waf_p,pr_unweighted_p,pr_normal_p,pr_waf_p,pn_p,pn_u_p,pn_waf_p = 0,0,0,0,0,0,0,0\n",
    "crp_normal_r,crp_waf_r,pr_unweighted_r,pr_normal_r,pr_waf_r,pn_r,pn_u_r,pn_waf_r = 0,0,0,0,0,0,0,0\n",
    "crp_normal_f,crp_waf_f,pr_unweighted_f,pr_normal_f,pr_waf_f,pn_f,pn_u_f,pn_waf_f = 0,0,0,0,0,0,0,0\n",
    "\n",
    "crp,crp_waf,prp_u,prp,prp_waf,pnp,pnp_u,pnp_waf = [],[],[],[],[],[],[],[]\n",
    "\n",
    "for window_size in range(3,15) :\n",
    "    print \"Window Size = \",window_size\n",
    "    crp_normal_p,crp_waf_p,pr_unweighted_p,pr_normal_p,pr_waf_p,pn_p,pn_u_p,pn_waf_p = 0,0,0,0,0,0,0,0\n",
    "    crp_normal_r,crp_waf_r,pr_unweighted_r,pr_normal_r,pr_waf_r,pn_r,pn_u_r,pn_waf_r = 0,0,0,0,0,0,0,0\n",
    "    crp_normal_f,crp_waf_f,pr_unweighted_f,pr_normal_f,pr_waf_f,pn_f,pn_u_f,pn_waf_f = 0,0,0,0,0,0,0,0\n",
    "    for filename in abstract_files :\n",
    "        filepath =  path+filename\n",
    "        with open(filepath) as file :\n",
    "            text = file.read()\n",
    "        text = \" \".join(text.strip().split())\n",
    "        tokens_stemmed,tokens_unstemmed = clean_text_simple(text)\n",
    "        #Traditional Graph of Words\n",
    "        G = terms_to_graph(tokens_stemmed,window_size)\n",
    "    #     H = nx.convert_node_labels_to_integers(G)\n",
    "    #     nx.write_edgelist(H,'edgelist.txt',comments='#', delimiter=' ', data=False, encoding='utf-8')\n",
    "\n",
    "        G.remove_edges_from(G.selfloop_edges())\n",
    "        core_number_normal_weighted =  core_dec(G)\n",
    "        peak_number_normal_unweighted = get_kpeak_decomposition(G,False)\n",
    "        peak_number_normal_weighted = get_kpeak_decomposition(G)\n",
    "        #Word Attraction Force Graph of Words\n",
    "        G_waf = terms_to_graph_word_attraction(tokens_stemmed,tokens_unstemmed,window_size)\n",
    "        G_waf.remove_edges_from(G_waf.selfloop_edges())\n",
    "        core_number_waf_weighted =  core_dec(G_waf)\n",
    "        peak_number_waf_weighted = get_kpeak_decomposition(G_waf)\n",
    "\n",
    "\n",
    "        core_rank_normal = compute_rank(core_number_normal_weighted,G)\n",
    "        peak_rank_normal = compute_rank(peak_number_normal_weighted,G)\n",
    "        peak_rank_unweighted = compute_rank(peak_number_normal_unweighted,G)\n",
    "        \n",
    "        \n",
    "        sorted_pn_weighted = sorted(peak_number_normal_weighted.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        sorted_pn_weighted = [x[0] for x in sorted_pn_weighted]\n",
    "        sorted_pn_unweighted = sorted(peak_number_waf_weighted.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        sorted_pn_unweighted = [x[0] for x in sorted_pn_unweighted]\n",
    "        sorted_pn_waf = sorted(peak_number_normal_unweighted.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        sorted_pn_waf = [x[0] for x in sorted_pn_waf]\n",
    "    \n",
    "        core_rank_waf = compute_rank(core_number_waf_weighted,G_waf)\n",
    "        peak_rank_waf = compute_rank(peak_number_waf_weighted,G_waf)\n",
    "\n",
    "\n",
    "        cr_n = set(core_rank_normal[:int(len(core_rank_normal)/3)])\n",
    "        cr_waf = set(core_rank_waf[:int(len(core_rank_waf)/3)])\n",
    "        pr_u = set(peak_rank_unweighted[:int(len(peak_number_normal_unweighted)/3)])\n",
    "        pr_n = set(peak_rank_normal[:int(len(core_rank_normal)/3)])\n",
    "        pr_waf = set(peak_rank_waf[:int(len(core_rank_waf)/3)])\n",
    "        \n",
    "        pn_w = set(sorted_pn_weighted[:int(len(sorted_pn_weighted)/3)])\n",
    "        pn_u = set(sorted_pn_weighted[:int(len(sorted_pn_unweighted)/3)])\n",
    "        pn_waf = set(sorted_pn_waf[:int(len(sorted_pn_waf)/3)])\n",
    "        \n",
    "        keywordspath = filepath.replace('abstr','uncontr')\n",
    "        with open(keywordspath) as file :\n",
    "            keywords = file.read()\n",
    "        keywords =  \" \".join(keywords.strip().split()).split(';')\n",
    "        keywords = [key.strip().split() for key in keywords]\n",
    "        keywords = set([stemmer.stem(word).lower() for key in keywords for word in key])\n",
    "        \n",
    "        crp_normal_p+=scores.precision(keywords,cr_n)\n",
    "        crp_normal_r+=scores.recall(keywords,cr_n)\n",
    "        crp_normal_f+=scores.f_measure(keywords,cr_n)\n",
    "\n",
    "        crp_waf_p+=scores.precision(keywords,cr_waf)\n",
    "        crp_waf_r+=scores.recall(keywords,cr_waf)\n",
    "        crp_waf_f+=scores.f_measure(keywords,cr_waf)\n",
    "        \n",
    "        pr_normal_p+=scores.precision(keywords,pr_n)\n",
    "        pr_normal_r+=scores.recall(keywords,pr_n)\n",
    "        pr_normal_f+=scores.f_measure(keywords,pr_n)\n",
    "\n",
    "        pr_waf_p+=scores.precision(keywords,pr_waf)\n",
    "        pr_waf_r+=scores.recall(keywords,pr_waf)\n",
    "        pr_waf_f+=scores.f_measure(keywords,pr_waf)\n",
    "        \n",
    "        pr_unweighted_p+=scores.precision(keywords,pr_u)\n",
    "        pr_unweighted_r+=scores.recall(keywords,pr_u)\n",
    "        pr_unweighted_f+=scores.f_measure(keywords,pr_u)\n",
    "\n",
    "        pn_p+=scores.precision(keywords,pn_w)\n",
    "        pn_r+=scores.recall(keywords,pn_w)\n",
    "        pn_f+=scores.f_measure(keywords,pn_w)\n",
    "\n",
    "\n",
    "        pn_u_p+=scores.precision(keywords,pn_u)\n",
    "        pn_u_r+=scores.recall(keywords,pn_u)\n",
    "        pn_u_f+=scores.f_measure(keywords,pn_u)\n",
    "\n",
    "        pn_waf_p+=scores.precision(keywords,pn_waf)\n",
    "        pn_waf_r+=scores.recall(keywords,pn_waf)\n",
    "        pn_waf_f+=scores.f_measure(keywords,pn_waf)\n",
    "                                \n",
    "    crp.append([crp_normal_f,crp_normal_p,crp_normal_r])\n",
    "    crp_waf.append([crp_waf_f,crp_waf_p,crp_waf_r])\n",
    "    prp.append([pr_normal_f,pr_normal_p,pr_normal_r])\n",
    "    prp_waf.append([pr_waf_f,pr_waf_p,pr_waf_r])\n",
    "    prp_u.append([pr_unweighted_f,pr_unweighted_p,pr_unweighted_r])\n",
    "    pnp.append([pn_f,pn_p,pn_r])\n",
    "    pnp_u.append([pn_u_f,pn_u_p,pn_u_r])\n",
    "    pnp_waf.append([pn_waf_f,pn_waf_p,pn_waf_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allp = [crp,crp_waf,prp_u,prp,prp_waf,pnp,pnp_u,pnp_waf]\n",
    "for x in allp :\n",
    "    a = sorted(x,reverse=True)[0]\n",
    "    print\n",
    "    for y in a :\n",
    "        print y/5,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
