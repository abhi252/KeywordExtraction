{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhna\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re \n",
    "import itertools\n",
    "import heapq\n",
    "import operator\n",
    "import networkx as nx\n",
    "import copy\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from operator import mul\n",
    "from nltk.metrics import scores\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clean and Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_simple(text, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
    "    \n",
    "    punct = string.punctuation.replace('-', '')\n",
    "    # remove punctuation (preserving intra-word dashes)\n",
    "    text = ''.join(l for l in text if l not in punct)\n",
    "    # strip extra white space\n",
    "    text = re.sub(' +',' ',text)\n",
    "    # strip leading and trailing white space\n",
    "    text = text.strip()\n",
    "    # tokenize (split based on whitespace)\n",
    "    tokens = text.split(' ')\n",
    "    if pos_filtering == True:\n",
    "        # apply POS-tagging\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        # retain only nouns and adjectives\n",
    "        tokens_keep = []\n",
    "        for i in range(len(tagged_tokens)):\n",
    "            item = tagged_tokens[i]\n",
    "            if (\n",
    "            item[1] == 'NN' or\n",
    "            item[1] == 'NNS' or\n",
    "            item[1] == 'NNP' or\n",
    "            item[1] == 'NNPS' or\n",
    "            item[1] == 'JJ' or\n",
    "            item[1] == 'JJS' or\n",
    "            item[1] == 'JJR'\n",
    "            ):\n",
    "                tokens_keep.append(item[0])\n",
    "        tokens = tokens_keep\n",
    "    if remove_stopwords:\n",
    "        stpwds = stopwords.words('english')\n",
    "        # remove stopwords\n",
    "        tokens = [token for token in tokens if token not in stpwds]\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        # apply Porter's stemmer\n",
    "        tokens_stemmed = list()\n",
    "        tokens_unstemmed = list()\n",
    "        for token in tokens:\n",
    "            tokens_stemmed.append(stemmer.stem(token).lower())\n",
    "            tokens_unstemmed.append(token)\n",
    "\n",
    "    return(tokens_stemmed,tokens_unstemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_graph(terms, w):\n",
    "    # This function returns a directed, weighted networkx graph from a list of terms (the tokens from the pre-processed text) e.g., ['quick','brown','fox']\n",
    "    # Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'\n",
    "    \n",
    "    from_to = {}\n",
    "    \n",
    "    # create initial complete graph (first w terms)\n",
    "    w = min(w,len(terms))\n",
    "    terms_temp = terms[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    \n",
    "    new_edges = []\n",
    "    \n",
    "    for my_tuple in indexes:\n",
    "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "    \n",
    "    for new_edge in new_edges:\n",
    "        if new_edge in from_to:\n",
    "            from_to[new_edge] += 1\n",
    "        else:\n",
    "            from_to[new_edge] = 1\n",
    "    \n",
    "    # then iterate over the remaining terms\n",
    "    for i in xrange(w, len(terms)):\n",
    "        # term to consider\n",
    "        considered_term = terms[i]\n",
    "        # all terms within sliding window\n",
    "        terms_temp = terms[(i-w+1):(i+1)]\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in xrange(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "            \n",
    "        for try_edge in candidate_edges:\n",
    "        \n",
    "            # if not self-edge\n",
    "            if try_edge[1] != try_edge[0]:\n",
    "                \n",
    "                # if edge has already been seen, update its weight\n",
    "                if try_edge in from_to:\n",
    "                    from_to[try_edge] += 1\n",
    "                \n",
    "                # if edge has never been seen, create it and assign it a unit weight     \n",
    "                else:\n",
    "                    from_to[try_edge] = 1\n",
    "    \n",
    "    # create empty graph\n",
    "    g = nx.DiGraph()\n",
    "    \n",
    "    for edge in from_to :\n",
    "        g.add_edge(edge[0],edge[1],weight=from_to[edge])\n",
    "    \n",
    "    degree_dict = g.degree(weight='weight')\n",
    "    nx.set_node_attributes(g,'weight',degree_dict)\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Attraction Force Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_vector_getter(word, word_vectors):\n",
    "    try:\n",
    "        word_representation = word_vectors[word].reshape(1,-1)\n",
    "        return (word_representation)\n",
    "    except KeyError:\n",
    "        return (np.random.uniform(-0.25,0.25,300).reshape(1,-1))\n",
    "        \n",
    "def my_euclidean_distance(word1, word2, word_vectors):\n",
    "    distance = ed(my_vector_getter(word1, word_vectors),my_vector_getter(word2, word_vectors))\n",
    "    return (round(distance, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAF Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_graph_word_attraction(terms_stemmed,terms_unstemmed,w):\n",
    "    # This function returns a directed, weighted networkx graph from a list of terms (the tokens from the pre-processed text) e.g., ['quick','brown','fox']\n",
    "    # Edges are weighted based on term co-occurence, word2vec vector and the respective frequencies within a sliding window of fixed size 'w'\n",
    "    from_to={}\n",
    "    \n",
    "    # create initial complete graph (first w terms)\n",
    "    w = min(w,len(terms_stemmed))\n",
    "    terms_temp = terms_stemmed[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    \n",
    "    new_edges = []\n",
    "    \n",
    "\n",
    "    for my_tuple in indexes:\n",
    "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "    \n",
    "    for new_edge in new_edges:\n",
    "        if new_edge in from_to:\n",
    "            from_to[new_edge] += 1\n",
    "        else:\n",
    "            from_to[new_edge] = 1\n",
    "    \n",
    "    # then iterate over the remaining terms\n",
    "    for i in xrange(w, len(terms_stemmed)):\n",
    "        # term to consider\n",
    "        considered_term = terms_stemmed[i]\n",
    "        # all terms within sliding window\n",
    "        terms_temp = terms_stemmed[(i-w+1):(i+1)]\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in xrange(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "            \n",
    "        for try_edge in candidate_edges:\n",
    "        \n",
    "            # if not self-edge\n",
    "            if try_edge[1] != try_edge[0]:\n",
    "                \n",
    "                # if edge has already been seen, update its weight\n",
    "                if try_edge in from_to:\n",
    "                    from_to[try_edge] += 1\n",
    "                \n",
    "                # if edge has never been seen, create it and assign it a unit weight     \n",
    "                else:\n",
    "                    from_to[try_edge] = 1\n",
    "    \n",
    "    # create empty graph\n",
    "    min_attr = float(\"inf\")\n",
    "    edgelist = from_to.keys()\n",
    "    waf = {}\n",
    "    for edge in edgelist :\n",
    "        word1 = edge[0]\n",
    "        word2 = edge[1]\n",
    "        if word1 != word2 :\n",
    "            word1_freq = terms_stemmed.count(word1)\n",
    "            word2_freq = terms_stemmed.count(word2)\n",
    "            word1_unstemmed = terms_unstemmed[terms_stemmed.index(word1)]\n",
    "            word2_unstemmed = terms_unstemmed[terms_stemmed.index(word2)]\n",
    "            distance = my_euclidean_distance(word1_unstemmed,word2_unstemmed,word_vectors)\n",
    "            force = round(word1_freq * word2_freq / float(distance * distance), 5)\n",
    "            dice = 2*from_to[edge]/(word1_freq*word2_freq)\n",
    "            attr = dice*force\n",
    "            if attr!=0:\n",
    "                min_attr = min(attr,min_attr)\n",
    "                waf[edge] = attr\n",
    "    \n",
    "    g = nx.DiGraph()\n",
    "    for item in waf :\n",
    "        if waf[item]!=0:\n",
    "            waf[item] = round(waf[item]*1.0/min_attr)\n",
    "            g.add_edge(item[0],item[1],weight=waf[item])\n",
    "    \n",
    "    degree_dict = g.degree(weight='weight')\n",
    "    nx.set_node_attributes(g,'weight',degree_dict)\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hulth Dataset Graph Contructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = os.listdir(\"EMNLP_2016-master/data/Hulth2003/validation_training/validation/\")\n",
    "path = \"EMNLP_2016-master/data/Hulth2003/validation_training/validation/\"\n",
    "abstract_files = [filename for filename in total if '.abstr' in filename]\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "for window_size in range(3,15) :\n",
    "    print \"Window Size = \",window_size\n",
    "    for filename in abstract_files :\n",
    "        filepath =  path+filename\n",
    "        with open(filepath) as file :\n",
    "            text = file.read()\n",
    "        text = \" \".join(text.strip().split())\n",
    "        tokens_stemmed,tokens_unstemmed = clean_text_simple(text)\n",
    "        #Traditional Graph of Words\n",
    "        G = terms_to_graph(tokens_stemmed,window_size)\n",
    "        G.remove_edges_from(G.selfloop_edges())\n",
    "        nodes = G.nodes()\n",
    "        H = nx.convert_node_labels_to_integers(G)\n",
    "        edgelist_file = \"Hulth-edgelists/\"+filename+\"_\"+str(window_size)+\"_normal.edgelist\"\n",
    "        edgelist_weighted_file = \"Hulth-edgelists/\"+filename+\"_\"+str(window_size)+\"_weighted.edgelist\"\n",
    "        edgelist_waf_file = \"Hulth-edgelists/\"+filename+\"_\"+str(window_size)+\"_waf.edgelist\"\n",
    "        nodelist_file = \"Hulth-edgelists/\"+filename+\"_\"+str(window_size)+\"_normal_node.pickle\"\n",
    "        nodelist_waf_file = \"Hulth-edgelists/\"+filename+\"_\"+str(window_size)+\"_waf_node.pickle\"\n",
    "        with open (nodelist_file,'wb') as f :\n",
    "            pickle.dump(nodes, f, pickle.HIGHEST_PROTOCOL)\n",
    "        nx.write_edgelist(H,edgelist_file,comments='#', data=False, encoding='utf-8')\n",
    "        nx.write_edgelist(H,edgelist_weighted_file,comments='#', data=True, encoding='utf-8')\n",
    "        \n",
    "        #Word Attraction Force Graph of Words\n",
    "        G_waf = terms_to_graph_word_attraction(tokens_stemmed,tokens_unstemmed,window_size)\n",
    "        G_waf.remove_edges_from(G_waf.selfloop_edges())\n",
    "        nodes = G_waf.nodes()\n",
    "        with open (nodelist_waf_file,'wb') as f :\n",
    "            pickle.dump(nodes, f, pickle.HIGHEST_PROTOCOL)\n",
    "        H = nx.convert_node_labels_to_integers(G_waf)\n",
    "        nx.write_edgelist(H,edgelist_waf_file,comments='#', delimiter=' ', data=True, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
